<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">








    






<link rel="icon" type="image/ico" href="https://siddhantmaharana.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://siddhantmaharana.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://siddhantmaharana.github.io/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="https://siddhantmaharana.github.io/android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://siddhantmaharana.github.io/apple-touch-icon.png">

<meta name="description" content=""/>



<title>
    
    AB Testing Fundamentals | Sid&#39;s Notes
    
</title>

<link rel="canonical" href="https://siddhantmaharana.github.io/posts/ab-testing-fundamentals/"/>

<meta property="og:url" content="https://siddhantmaharana.github.io/posts/ab-testing-fundamentals/">
  <meta property="og:site_name" content="Sid&#39;s Notes">
  <meta property="og:title" content="AB Testing Fundamentals">
  <meta property="og:description" content="I’ve been running A/B tests for a while now, and I keep coming back to these fundamental concepts. Writing them down helps me think through them more clearly - maybe they’ll be useful for you too.
What A/B Testing Actually Is # A/B testing is basically a controlled experiment to figure out which version of something works better. The key word here is “controlled” - you’re not just comparing random things, you’re systematically testing one change at a time.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2022-12-28T16:24:23-04:00">
    <meta property="article:modified_time" content="2022-12-28T16:24:23-04:00">
    <meta property="article:tag" content="Experimentation">
    <meta property="article:tag" content="Statistics">












<link rel="stylesheet" href="/assets/combined.min.92c3bf7119b98cfdc79e93f36a451eb901d8bbbfed7d75814e6436cf6c9085dc.css" media="all">




      <script async src="https://www.googletagmanager.com/gtag/js?id=G-xxxxxxxxx"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-xxxxxxxxx');
        }
      </script>











    




</head>







<body class="auto">

  <div class="content">
    <header>
      

<div class="header">

    

    <h1 class="header-title">
        <a href="https://siddhantmaharana.github.io/">Sid&#39;s Notes</a>
    </h1>

    <div class="header-menu">
        

        
        

        <p
            class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        

        <p
            class="small ">
            <a href="/posts" >
                /posts
            </a>
        </p>
        

        <p
            class="small ">
            <a href="/about" >
                /about
            </a>
        </p>
        
        
    </div>

    

</div>

    </header>

    <main class="main">
      




<div class="breadcrumbs"><a href="/">Home</a><span class="breadcrumbs-separator">/</span><a href="/posts/">Posts</a><span class="breadcrumbs-separator">/</span>
        <a href="/posts/ab-testing-fundamentals/">AB Testing Fundamentals</a></div>


<div >
  <article>
    <header class="single-intro-container">
        
        <h1 class="single-title">AB Testing Fundamentals</h1>
        
        <div class="single-subsummary">
          
          <div>
            
            <p class="single-date">
              <time datetime="2022-12-28T16:24:23-04:00">December 28, 2022</time>
            </p>
          </div>
        </div>
        
    </header>
    
    <div class="single-content">
      <p>I&rsquo;ve been running A/B tests for a while now, and I keep coming back to these fundamental concepts. Writing them down helps me think through them more clearly - maybe they&rsquo;ll be useful for you too.</p>
<h2 class="heading" id="what-ab-testing-actually-is">
  What A/B Testing Actually Is
  <a class="anchor" href="#what-ab-testing-actually-is">#</a>
</h2>
<p>A/B testing is basically a controlled experiment to figure out which version of something works better. The key word here is &ldquo;controlled&rdquo; - you&rsquo;re not just comparing random things, you&rsquo;re systematically testing one change at a time.</p>
<p>The core characteristics that make it work:</p>
<ul>
<li><strong>Two variants</strong>: Control (what you have now) and treatment (what you&rsquo;re testing)</li>
<li><strong>Random assignment</strong>: Users get randomly bucketed into groups</li>
<li><strong>Measurement</strong>: You track specific metrics to see what happens</li>
</ul>
<p>That random assignment piece is crucial. Without it, you&rsquo;re just looking at correlations, and we all know correlation doesn&rsquo;t equal causation.</p>
<h3 class="heading" id="why-random-assignment-matters">
  Why Random Assignment Matters
  <a class="anchor" href="#why-random-assignment-matters">#</a>
</h3>
<p>Random assignment does three important things:</p>
<ol>
<li><strong>Creates representative groups</strong>: When done right, your test groups should look like mini versions of your broader user base</li>
<li><strong>Minimizes bias</strong>: Both known and unknown variables get evenly distributed between groups</li>
<li><strong>Establishes causality</strong>: By controlling for everything except your one change, you can actually say that change caused the difference you observed</li>
</ol>
<h3 class="heading" id="control-vs-treatment">
  Control vs Treatment
  <a class="anchor" href="#control-vs-treatment">#</a>
</h3>
<p>This is straightforward but worth being explicit about:</p>
<ul>
<li><strong>Control</strong>: The current state, your baseline, what users see today</li>
<li><strong>Treatment</strong>: The new thing you&rsquo;re testing, hopefully an improvement</li>
</ul>
<h2 class="heading" id="when-ab-testing-makes-sense-and-when-it-doesnt">
  When A/B Testing Makes Sense (And When It Doesn&rsquo;t)
  <a class="anchor" href="#when-ab-testing-makes-sense-and-when-it-doesnt">#</a>
</h2>
<p>A/B testing is great for:</p>
<p><strong>Conversion Rate Optimization</strong></p>
<ul>
<li>Testing checkout flows, signup processes, email campaigns</li>
<li>Optimizing calls-to-action, form designs, landing pages</li>
</ul>
<p><strong>Feature Releases</strong></p>
<ul>
<li>Rolling out new functionality gradually</li>
<li>Validating UI/UX changes before full launch</li>
</ul>
<p><strong>Product Impact Measurement</strong></p>
<ul>
<li>Understanding how changes affect key business metrics</li>
<li>Testing marketing campaigns and pricing strategies</li>
</ul>
<p>But it&rsquo;s not always the right tool:</p>
<p><strong>When you don&rsquo;t have enough traffic</strong> - If you can&rsquo;t reach statistical significance in a reasonable timeframe, don&rsquo;t bother</p>
<p><strong>Ethical concerns</strong> - Some tests could harm users or feel manipulative</p>
<p><strong>No clear hypothesis</strong> - If you can&rsquo;t articulate what you expect to happen and why, you&rsquo;re not ready to test</p>
<p><strong>Resource constraints</strong> - Sometimes the cost of setting up and running the test outweighs the potential benefit</p>
<h2 class="heading" id="setting-up-tests-properly">
  Setting Up Tests Properly
  <a class="anchor" href="#setting-up-tests-properly">#</a>
</h2>
<h3 class="heading" id="the-process-i-follow">
  The Process I Follow
  <a class="anchor" href="#the-process-i-follow">#</a>
</h3>
<ol>
<li><strong>Define the goal clearly</strong> - What are you trying to improve and why?</li>
<li><strong>Form a testable hypothesis</strong> - &ldquo;Based on X observation, if we change Y, then Z will happen because of reason R&rdquo;</li>
<li><strong>Choose your metrics</strong> - Primary metric (the thing you care most about), secondary metrics (other things worth tracking), guardrail metrics (things that shouldn&rsquo;t get worse)</li>
<li><strong>Calculate sample size</strong> - How many users do you need to detect a meaningful difference?</li>
<li><strong>Run the test</strong> - Resist the urge to peek at results early</li>
<li><strong>Analyze and act</strong> - Look at the data, make a decision, document what you learned</li>
</ol>
<h3 class="heading" id="picking-good-metrics">
  Picking Good Metrics
  <a class="anchor" href="#picking-good-metrics">#</a>
</h3>
<p>I think about metrics in three categories:</p>
<p><strong>Primary/North Star Metrics</strong>: The big picture stuff that directly reflects business value. Revenue per visitor, conversion rate, user retention.</p>
<p><strong>Granular Metrics</strong>: More specific user behaviors that are easier to move and give you insight into what&rsquo;s happening. Click-through rate, time on page, signup rate.</p>
<p><strong>Guardrail Metrics</strong>: Things you don&rsquo;t want to break while optimizing other things. Site speed, error rates, user satisfaction scores.</p>
<p>A good metric is:</p>
<ul>
<li><strong>Stable</strong>: Doesn&rsquo;t fluctuate wildly for no reason</li>
<li><strong>Sensitive</strong>: Actually responds when you make meaningful changes</li>
<li><strong>Measurable</strong>: You can track it accurately and consistently</li>
<li><strong>Non-gameable</strong>: Hard to manipulate without creating real value</li>
</ul>
<h2 class="heading" id="understanding-statistical-concepts">
  Understanding Statistical Concepts
  <a class="anchor" href="#understanding-statistical-concepts">#</a>
</h2>
<h3 class="heading" id="the-math-you-need-to-know">
  The Math You Need to Know
  <a class="anchor" href="#the-math-you-need-to-know">#</a>
</h3>
<p><strong>Statistical Significance (α)</strong>: Usually set at 5%. This is your false positive rate - how often you&rsquo;ll think there&rsquo;s a difference when there isn&rsquo;t one.</p>
<p><strong>Statistical Power (1-β)</strong>: Usually set at 80%. This is your ability to detect a real difference when it exists.</p>
<p><strong>Minimum Detectable Effect (MDE)</strong>: The smallest change you care about detecting. Smaller effects require bigger sample sizes.</p>
<p>The relationship between these determines your sample size. You can&rsquo;t optimize all three - if you want to detect smaller effects with higher confidence, you need more data.</p>
<h3 class="heading" id="effect-size-and-cohens-measures">
  Effect Size and Cohen&rsquo;s Measures
  <a class="anchor" href="#effect-size-and-cohens-measures">#</a>
</h3>
<p>Effect size tells you not just whether there&rsquo;s a difference, but how big that difference is. Cohen&rsquo;s measures standardize this:</p>
<p><strong>For proportions (like conversion rates)</strong>: Cohen&rsquo;s h</p>
<ul>
<li>Formula: h = 2 × (arcsin(√P₂) - arcsin(√P₁))</li>
</ul>
<p><strong>For means (like revenue per user)</strong>: Cohen&rsquo;s d</p>
<ul>
<li>Formula: d = (μ₂ - μ₁)/σ</li>
</ul>
<p>The interpretation is the same for both:</p>
<ul>
<li>0.2 = Small effect</li>
<li>0.5 = Medium effect</li>
<li>0.8 = Large effect</li>
</ul>
<h3 class="heading" id="sample-size-calculation">
  Sample Size Calculation
  <a class="anchor" href="#sample-size-calculation">#</a>
</h3>
<p>Once you have your effect size, you can calculate sample size:
n = 2(Zα + Zβ)² / effect_size²</p>
<p>Example: Testing a checkout flow change</p>
<ul>
<li>Baseline conversion: 10%</li>
<li>Target conversion: 11% (10% relative improvement)</li>
<li>Cohen&rsquo;s h ≈ 0.033 (small effect)</li>
<li>Sample size needed: ~14,728 per variant</li>
<li>With 5,000 daily visitors: ~6 day test</li>
</ul>
<h2 class="heading" id="common-pitfalls-and-how-to-avoid-them">
  Common Pitfalls and How to Avoid Them
  <a class="anchor" href="#common-pitfalls-and-how-to-avoid-them">#</a>
</h2>
<h3 class="heading" id="multiple-comparisons-problem">
  Multiple Comparisons Problem
  <a class="anchor" href="#multiple-comparisons-problem">#</a>
</h3>
<p>When you test multiple metrics simultaneously, your false positive rate inflates. If you test 4 metrics at 5% significance each, your overall false positive rate jumps to about 19%.</p>
<p>Solutions:</p>
<ul>
<li><strong>Bonferroni correction</strong>: Divide your significance level by number of tests</li>
<li><strong>Hierarchy approach</strong>: Have one primary metric, treat others as secondary</li>
<li><strong>Accept the tradeoff</strong>: Sometimes a slightly higher false positive rate is acceptable</li>
</ul>
<h3 class="heading" id="sample-ratio-mismatch-srm">
  Sample Ratio Mismatch (SRM)
  <a class="anchor" href="#sample-ratio-mismatch-srm">#</a>
</h3>
<p>This happens when your user split deviates from what you planned (e.g., 48/52 instead of 50/50). It usually indicates a problem with your randomization.</p>
<p>Check for SRM using a chi-square test. If you find it, investigate:</p>
<ul>
<li>Randomization algorithm bugs</li>
<li>Delayed start times for variants</li>
<li>Data logging issues</li>
<li>Bot filtering differences</li>
</ul>
<h3 class="heading" id="peeking-at-results">
  Peeking at Results
  <a class="anchor" href="#peeking-at-results">#</a>
</h3>
<p>Every time you check results before reaching your planned sample size, you increase your false positive rate. What feels like 5% significance becomes ~14% if you peek 5 times.</p>
<p>Solutions:</p>
<ul>
<li>Set a test duration and stick to it</li>
<li>Use sequential analysis methods if you must monitor</li>
<li>Only stop early for dramatic effects or serious problems</li>
</ul>
<h3 class="heading" id="external-validity-issues">
  External Validity Issues
  <a class="anchor" href="#external-validity-issues">#</a>
</h3>
<p><strong>Simpson&rsquo;s Paradox</strong>: Results can flip when you segment your data. Always check if your overall results hold within important user segments.</p>
<p><strong>Novelty Effects</strong>: Users might try new things just because they&rsquo;re new. Run tests long enough for behavior to stabilize.</p>
<p><strong>Change Aversion</strong>: The opposite problem - users might avoid new features initially even if they&rsquo;re better.</p>
<h2 class="heading" id="advanced-considerations">
  Advanced Considerations
  <a class="anchor" href="#advanced-considerations">#</a>
</h2>
<h3 class="heading" id="ratio-metrics-and-the-delta-method">
  Ratio Metrics and the Delta Method
  <a class="anchor" href="#ratio-metrics-and-the-delta-method">#</a>
</h3>
<p>When your metric is a ratio (like revenue per user), standard t-tests don&rsquo;t work properly because the numerator and denominator can vary independently.</p>
<p>The delta method accounts for this by considering the covariance between numerator and denominator. For a ratio R = X/Y:</p>
<p>Var(R) ≈ (1/μy²)[σx² + R²σy² - 2Rσxy]</p>
<p>This matters because two scenarios with the same ratio can have very different confidence intervals depending on the underlying variance.</p>
<h3 class="heading" id="handling-multiple-metrics">
  Handling Multiple Metrics
  <a class="anchor" href="#handling-multiple-metrics">#</a>
</h3>
<p>In practice, you usually care about more than one metric. Here&rsquo;s how I approach it:</p>
<ol>
<li><strong>Establish hierarchy</strong>: One primary metric, several secondary ones, guardrails</li>
<li><strong>Account for correlations</strong>: Related metrics will move together</li>
<li><strong>Set decision rules upfront</strong>: What combination of results will make you ship?</li>
<li><strong>Use statistical corrections when appropriate</strong>: But don&rsquo;t be overly conservative</li>
</ol>
<h2 class="heading" id="practical-implementation">
  Practical Implementation
  <a class="anchor" href="#practical-implementation">#</a>
</h2>
<h3 class="heading" id="infrastructure-youll-need">
  Infrastructure You&rsquo;ll Need
  <a class="anchor" href="#infrastructure-youll-need">#</a>
</h3>
<p><strong>User Assignment</strong>:</p>
<ul>
<li>Reliable randomization algorithm</li>
<li>Consistent user identification (cookies, user IDs)</li>
<li>Proper traffic allocation controls</li>
</ul>
<p><strong>Data Collection</strong>:</p>
<ul>
<li>Event logging for all relevant metrics</li>
<li>Data quality monitoring</li>
<li>Real-time dashboards for monitoring</li>
</ul>
<p><strong>Analysis Tools</strong>:</p>
<ul>
<li>Statistical testing capabilities</li>
<li>Confidence interval calculations</li>
<li>Segmentation and drilling-down features</li>
</ul>
<h3 class="heading" id="quality-assurance">
  Quality Assurance
  <a class="anchor" href="#quality-assurance">#</a>
</h3>
<p><strong>A/A Tests</strong>: Run identical experiences to two groups to validate your setup. If you see significant differences, you have a problem.</p>
<p><strong>Balance Checks</strong>: Verify that your randomization worked by checking that user characteristics are balanced between groups.</p>
<p><strong>Data Quality Monitoring</strong>: Watch for unusual patterns, missing data, or technical issues that could bias results.</p>
<h2 class="heading" id="making-decisions">
  Making Decisions
  <a class="anchor" href="#making-decisions">#</a>
</h2>
<p>At the end of the day, A/B testing is about making better decisions. Here&rsquo;s my framework:</p>
<ol>
<li><strong>Statistical significance</strong>: Is the difference real or likely due to chance?</li>
<li><strong>Practical significance</strong>: Is the difference big enough to matter?</li>
<li><strong>Business context</strong>: Does this align with your strategy and priorities?</li>
<li><strong>Risk assessment</strong>: What are the downsides if you&rsquo;re wrong?</li>
<li><strong>Implementation cost</strong>: Is the benefit worth the effort to build and maintain?</li>
</ol>
<h2 class="heading" id="key-takeaways">
  Key Takeaways
  <a class="anchor" href="#key-takeaways">#</a>
</h2>
<p>A/B testing is a powerful tool, but it&rsquo;s not magic. It requires:</p>
<ul>
<li>Clear thinking about what you&rsquo;re trying to learn</li>
<li>Proper statistical setup and execution</li>
<li>Careful interpretation of results</li>
<li>Good judgment about when and how to act</li>
</ul>
<p>The math is important, but the thinking is more important. Start with a clear hypothesis, design a clean test, and be honest about what the data tells you.</p>
<p>Most importantly: A/B testing is about learning, not just optimizing. Even &ldquo;failed&rdquo; tests teach you something valuable about your users and your product.</p>

    </div>
  </article>

  

  

  
  

<div class="single-pagination">
    <hr />

    <div class="flexnowrap">

        <div class="single-pagination-prev">
            
            <div class="single-pagination-container-prev">
                <div class="single-pagination-text">←</div>
                <div class="single-pagination-text">
                    <a href="/posts/inferential-stats/">
                        Notes on Inferential Stats
                    </a>
                </div>
            </div>
            
        </div>

        <div class="single-pagination-next">
            
        </div>

    </div>

    <hr />
</div>



  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


    </main>
  </div>

  
  





    




  <footer>
    

    
    





    




    
    <p>Powered by
        <a href="https://gohugo.io/">Hugo</a>
        and
        <a href="https://github.com/tomfran/typo">tomfran/typo</a>
    </p>
    
    
    


  </footer>

  
</body>

<script src="/js/theme-switch.js"></script>
<script defer src="/js/copy-code.js"></script>
</html>
