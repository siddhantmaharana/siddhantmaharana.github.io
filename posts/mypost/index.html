<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=58961&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">








    






<link rel="icon" type="image/ico" href="http://localhost:58961/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:58961/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:58961/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:58961/android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:58961/apple-touch-icon.png">

<meta name="description" content=""/>



<title>
    
    AB Testing Fundamentals | Sid&#39;s Blog
    
</title>

<link rel="canonical" href="http://localhost:58961/posts/mypost/"/>

<meta property="og:url" content="http://localhost:58961/posts/mypost/">
  <meta property="og:site_name" content="Sid&#39;s Blog">
  <meta property="og:title" content="AB Testing Fundamentals">
  <meta property="og:description" content="Basic Concepts # What is A/B testing, and how does it differ from other types of experiments? # A/B testing is an experimentatal design which is set up to decide which version is better based on certain metrics. One of the core characteristic of an A/B test is that, it should have two variants- Control variant, which is the current state or design, and a treatment variant, which is the new design that is being tested. Random assigment is another key characteristic of an A/B test, where the users are randomly enrolled in the experiment. Participants are randomly assigned to either control or treatment variant, and this randomization is crucial in minimizing bias and establishing causality. What is the role of random assignment in A/B testing? # Randomization creates generalizability and representiveness, by ensuring that the test groups reflect the broader population and the test results can be applied to general user base. Randomization also ensures that it minimizes bias between different groups. This means that the known and the unknown variables would have an equal distribution between different groups, and thus there is no systematic difference between the groups. It also helps to establish causality better by isolating the variable being tested and eliminating the confounding variables. Explain the difference between control variant and treatment variant. # Control variant, or the “current state” represents the existing version, usually the version which is in production or in use. It serves as the benchmark against which the changes are measured. Treatment variant is the “new design” and represents the modified version that is being tested. This version is potentially hypothesized to perform better. Why is it important to have a control group in A/B testing? # A/B testing helps to reduce uncertainty around the impact of new features or designs. And, without a control group, we can’t isolate if the changes are due to the new design/feature, or external factors or just random fluctuations. A control group thus helps by providing a baseline, and isolating the treatment effect. Following which, we can carry out a proper statistical comparison. How does A/B testing help in making data-driven decisions? # A/B testing allows scientific evidence based decision making, which are not ‘based on intuition’ It helps in establishing clear business impacts by tracking specific metrics(such as North start, granular) and quantifies the effect of changes in terms of business value A/B testing enables risk optimization as well, by testing features before roll out, enabling continuous improvement and, thus reducing uncertainty in business decision making Use Cases &amp; Applications # What are three primary business scenarios where A/B testing is most valuable? # Some use cases in businesses include:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-28T16:24:23-04:00">
    <meta property="article:modified_time" content="2024-12-28T16:24:23-04:00">












<link rel="stylesheet" href="/assets/combined.min.92c3bf7119b98cfdc79e93f36a451eb901d8bbbfed7d75814e6436cf6c9085dc.css" media="all">











    




</head>







<body class="auto">

  <div class="content">
    <header>
      

<div class="header">

    

    <h1 class="header-title">
        <a href="http://localhost:58961/">Sid&#39;s Blog</a>
    </h1>

    <div class="header-menu">
        

        
        

        <p
            class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        

        <p
            class="small ">
            <a href="/posts" >
                /posts
            </a>
        </p>
        

        <p
            class="small ">
            <a href="/about" >
                /about
            </a>
        </p>
        
        
    </div>

    

</div>

    </header>

    <main class="main">
      




<div class="breadcrumbs"><a href="/">Home</a><span class="breadcrumbs-separator">/</span><a href="/posts/">Posts</a><span class="breadcrumbs-separator">/</span>
        <a href="/posts/mypost/">AB Testing Fundamentals</a></div>


<div >
  <article>
    <header class="single-intro-container">
        
        <h1 class="single-title">AB Testing Fundamentals</h1>
        
        <div class="single-subsummary">
          
          <div>
            
            <p class="single-date">
              <time datetime="2024-12-28T16:24:23-04:00">December 28, 2024</time>
            </p>
          </div>
        </div>
        
    </header>
    
    <div class="single-content">
      <h2 class="heading" id="basic-concepts">
  Basic Concepts
  <a class="anchor" href="#basic-concepts">#</a>
</h2>
<h3 class="heading" id="what-is-ab-testing-and-how-does-it-differ-from-other-types-of-experiments">
  What is A/B testing, and how does it differ from other types of experiments?
  <a class="anchor" href="#what-is-ab-testing-and-how-does-it-differ-from-other-types-of-experiments">#</a>
</h3>
<ul>
<li>A/B testing is an experimentatal design which is set up to decide which version is better based on certain metrics.</li>
<li>One of the core characteristic of an A/B test is that, it should have two variants- Control variant, which is the current state or design, and a treatment variant, which is the new design that is being tested.</li>
<li>Random assigment is another key characteristic of an A/B test, where the users are randomly enrolled in the experiment. Participants are randomly assigned to either control or treatment variant, and this randomization is crucial in minimizing bias and establishing causality.</li>
</ul>
<h3 class="heading" id="what-is-the-role-of-random-assignment-in-ab-testing">
  What is the role of random assignment in A/B testing?
  <a class="anchor" href="#what-is-the-role-of-random-assignment-in-ab-testing">#</a>
</h3>
<ul>
<li>Randomization creates generalizability and representiveness, by ensuring that the test groups reflect the broader population and the test results can be applied to general user base.</li>
<li>Randomization also ensures that it minimizes bias between different groups. This means that the known and the unknown variables would have an equal distribution between different groups, and thus there is no systematic difference between the groups.</li>
<li>It also helps to establish causality better by isolating the variable being tested and eliminating the confounding variables.</li>
</ul>
<h3 class="heading" id="explain-the-difference-between-control-variant-and-treatment-variant">
  Explain the difference between control variant and treatment variant.
  <a class="anchor" href="#explain-the-difference-between-control-variant-and-treatment-variant">#</a>
</h3>
<ul>
<li>Control variant, or the &ldquo;current state&rdquo; represents the existing version, usually the version which is in production or in use. It serves as the benchmark against which the changes are measured.</li>
<li>Treatment variant is the &ldquo;new design&rdquo; and represents the modified version that is being tested. This version is potentially hypothesized to perform better.</li>
</ul>
<h3 class="heading" id="why-is-it-important-to-have-a-control-group-in-ab-testing">
  Why is it important to have a control group in A/B testing?
  <a class="anchor" href="#why-is-it-important-to-have-a-control-group-in-ab-testing">#</a>
</h3>
<ul>
<li>A/B testing helps to reduce uncertainty around the impact of new features or designs. And, without a control group, we can&rsquo;t isolate if the changes are due to the new design/feature, or external factors or just random fluctuations.</li>
<li>A control group thus helps by providing a baseline, and isolating the treatment effect. Following which, we can carry out a proper statistical comparison.</li>
</ul>
<h3 class="heading" id="how-does-ab-testing-help-in-making-data-driven-decisions">
  How does A/B testing help in making data-driven decisions?
  <a class="anchor" href="#how-does-ab-testing-help-in-making-data-driven-decisions">#</a>
</h3>
<ul>
<li>A/B testing allows scientific evidence based decision making, which are not &lsquo;based on intuition&rsquo;</li>
<li>It helps in establishing clear business impacts by tracking specific metrics(such as North start, granular) and quantifies the effect of changes in terms of business value</li>
<li>A/B testing enables risk optimization as well, by testing features before roll out, enabling continuous improvement and, thus reducing uncertainty in business decision making</li>
</ul>
<h2 class="heading" id="use-cases--applications">
  Use Cases &amp; Applications
  <a class="anchor" href="#use-cases--applications">#</a>
</h2>
<h3 class="heading" id="what-are-three-primary-business-scenarios-where-ab-testing-is-most-valuable">
  What are three primary business scenarios where A/B testing is most valuable?
  <a class="anchor" href="#what-are-three-primary-business-scenarios-where-ab-testing-is-most-valuable">#</a>
</h3>
<p>Some use cases in businesses include:</p>
<ol>
<li>Conversion Rate Optimization
<ul>
<li>Improving conversion rates in checkout processes or signup flows</li>
<li>Testing app or web functionalities to increase user conversions</li>
<li>Optimizing marketing funnels and improving conversion rates for emails or notifications</li>
</ul>
</li>
<li>New Feature Releases
<ul>
<li>Adding new functionalities and testing before rolling out</li>
<li>Validatin new UI/UX</li>
</ul>
</li>
<li>Measuring Product Impact
<ul>
<li>Assessing and measuring impact on key metrics during product changes</li>
<li>Testing marketing campaigns, pricing and other strategies.</li>
</ul>
</li>
</ol>
<h3 class="heading" id="list-situations-where-ab-testing-might-not-be-appropriate-or-necessary">
  List situations where A/B testing might not be appropriate or necessary.
  <a class="anchor" href="#list-situations-where-ab-testing-might-not-be-appropriate-or-necessary">#</a>
</h3>
<p>Despite a lot of use cases to establish causality, there can be certain situations where A/B testing might not be appropriate or necessary</p>
<ol>
<li>Insufficient data
<ul>
<li>There can be situations where the traffic volume is quite low or the sample size is too less</li>
<li>Less traffic would cause the test to not reach enough statistical significance</li>
</ul>
</li>
<li>Ethical considerations
<ul>
<li>There can be instances, where testing a certain feature could harm user experience</li>
<li>Other instances where experiment on pricing can be considered ethically inappropriate</li>
</ul>
</li>
<li>Lack of a clear hypothesis
<ul>
<li>If there&rsquo;s a lack of a clear hypothesis, then A/B testing can be tricky</li>
<li>Or, there are no clear metrics to measure</li>
</ul>
</li>
<li>Resource constraints
<ul>
<li>This can happen when the cost of each sample is significant for example.</li>
<li>This can also happen when there is not enough technical infrastructure to set up an A/B test</li>
<li>High opportunity cost</li>
</ul>
</li>
</ol>
<h3 class="heading" id="how-can-ab-testing-be-used-in-the-follwing-e-commerce-optimization-drug-trials-feature-releases-marketing-campaigns">
  How can A/B testing be used in the follwing: e-commerce optimization, drug trials, feature releases, marketing campaigns?
  <a class="anchor" href="#how-can-ab-testing-be-used-in-the-follwing-e-commerce-optimization-drug-trials-feature-releases-marketing-campaigns">#</a>
</h3>
<ul>
<li>E commerce optimizations:
<ul>
<li>Testing checkout flow variations</li>
<li>Testing product recommendations</li>
<li>Testing UI/UX changes</li>
</ul>
</li>
<li>Drug Trials
<ul>
<li>Measuring efficacy of a drug</li>
<li>Controlling for placebo effects</li>
<li>Measuring specific health outcomes</li>
</ul>
</li>
<li>Feature releases
<ul>
<li>Managing gradual release of new features</li>
<li>Measuring UI/UX changes</li>
<li>Validating performance improvements</li>
</ul>
</li>
<li>Marketing Campaigns
<ul>
<li>Testing ad copy or designs</li>
<li>Comparing email campaign variations</li>
<li>Optimizing landing pages</li>
</ul>
</li>
</ul>
<h3 class="heading" id="what-role-does-sample-size-play-in-determining-whether-to-conduct-an-ab-test">
  What role does sample size play in determining whether to conduct an A/B test?
  <a class="anchor" href="#what-role-does-sample-size-play-in-determining-whether-to-conduct-an-ab-test">#</a>
</h3>
<p>Sample sizes can plays a crucial role in A/B test decisions</p>
<ul>
<li>Insufficient samples makes it impossible to detect true differences and can lead to unreliable results</li>
<li>Too small sample sizes waste resources on inconclusive tests</li>
<li>Inadequate samples can lead to incorrect business decisions</li>
</ul>
<h3 class="heading" id="how-do-ethical-considerations-impact-ab-testing-decisions">
  How do ethical considerations impact A/B testing decisions?
  <a class="anchor" href="#how-do-ethical-considerations-impact-ab-testing-decisions">#</a>
</h3>
<p>Some tests can be technically possible but ethically inappropriate</p>
<ul>
<li>In terms of user experience and safety, there can be certain tests that can put users at risk, for example, testing medicine dosages</li>
<li>Testing on vulnerable population without safeguards can be ethically inappropriate as well</li>
<li>Testing deceptive messaging, discriminatory pricing or hiding important information from users can also be considered as unethical business practices</li>
</ul>
<h2 class="heading" id="experimental-design">
  Experimental Design
  <a class="anchor" href="#experimental-design">#</a>
</h2>
<h3 class="heading" id="what-are-the-fundamental-steps-in-conducting-an-ab-test">
  What are the fundamental steps in conducting an A/B test?
  <a class="anchor" href="#what-are-the-fundamental-steps-in-conducting-an-ab-test">#</a>
</h3>
<p>These can be some of the fundamental steps necessary to conduct an A/B test</p>
<ol>
<li>Goal and design specification
<ul>
<li>Define a clear objective and a clear hypothesis to test</li>
<li>Define a success criteria and have clear metrics to measure. For example, there can be primary and secondary metrics in addition to some monitoring or guardrail metrics</li>
</ul>
</li>
<li>Random sampling
<ul>
<li>Determine the required sample size for the experiment</li>
<li>Ensure that the sample is representative of the population</li>
<li>Also, things like duration, timing should be kept in mind</li>
</ul>
</li>
<li>Random assignment
<ul>
<li>Ensure that the randomization algorithm works to split the user into control and treatment groups</li>
<li>Verify the balanced distributions</li>
</ul>
</li>
<li>Data collection
<ul>
<li>Once the test has been initiated, ensure that the logging is in place and correctly track the necessary metrics</li>
<li>Ensure that the data is consistent and check for data quality issues, such as balance or SRM issues</li>
</ul>
</li>
<li>Statistical analysis
<ul>
<li>After the test duration is over, collect the metrics for both the groups and test for statistical significant differences</li>
<li>Account for any confounding variables</li>
<li>Draw conclusions based on the statistical tests</li>
</ul>
</li>
</ol>
<h3 class="heading" id="what-factors-should-be-considered-when-designing-an-ab-test">
  What factors should be considered when designing an A/B test?
  <a class="anchor" href="#what-factors-should-be-considered-when-designing-an-ab-test">#</a>
</h3>
<ul>
<li>Key factors for AB test design include, sample considerations(size and representation), metric selection(primary, secondary and guardrail) and test parameters(test and control)</li>
<li>Practical constraints such as technical feasibility and resource availability should also be considered. Statistical rigor through significance levels and power analysis should be considered</li>
<li>Finally, business risks, ethical considerations and clear success criteria should be considered.</li>
</ul>
<h3 class="heading" id="how-do-you-determine-the-duration-of-an-ab-test">
  How do you determine the duration of an A/B test?
  <a class="anchor" href="#how-do-you-determine-the-duration-of-an-ab-test">#</a>
</h3>
<ul>
<li>The duration of an AB test can be determined by following three factors, required sample size to achieve statistical significance, expected traffic or user volume and business cycle considerations.</li>
<li>There should be enough time to collect sufficient data points, while accounting for any seasonal variations or cyclical variations in the users behavior.</li>
</ul>
<h2 class="heading" id="randomization--causality">
  Randomization &amp; Causality
  <a class="anchor" href="#randomization--causality">#</a>
</h2>
<h3 class="heading" id="what-is-the-relationship-between-randomization-and-bias-minimization">
  What is the relationship between randomization and bias minimization?
  <a class="anchor" href="#what-is-the-relationship-between-randomization-and-bias-minimization">#</a>
</h3>
<ul>
<li>Randomization helps to minimize bias by ensuring that any systematic differences or confounding variables are evenly distributed between control and treatment groups. When users are randomly assigned, the factors that can influence the outcome, both known and unknown have an equal chance in appearing in either of the groups.</li>
<li>This makes randomization a critical tool for creating comparable groups and isolating the true effect of the treatment being tested.</li>
</ul>
<h3 class="heading" id="what-are-potential-threats-to-randomization-in-ab-testing">
  What are potential threats to randomization in A/B testing?
  <a class="anchor" href="#what-are-potential-threats-to-randomization-in-ab-testing">#</a>
</h3>
<p>There can be potential threats to randomization. Some of them include:</p>
<ul>
<li>Selection Bias: Users can self-select themselves into groups. This can also happen because of certain time based assignment patterns, or geographic or device patterns.</li>
<li>Technical Issues: Cookie deletion after assignment can also cause users to move to different groups. There can be other challenges like, user using multiple devices, that can also lead to randomization issues.</li>
<li>Implementation Issues: Issues with Randomization algorithm, inconsitent assignment rules, or traffic allocation issues can also cause randomization problems.</li>
<li>External factors: Then there can be seasonal variations, or market events, or even changes in user behavior that can cause issues with randomization.</li>
</ul>
<h3 class="heading" id="how-does-proper-randomization-differ-from-pseudo-random-assignment">
  How does proper randomization differ from pseudo-random assignment?
  <a class="anchor" href="#how-does-proper-randomization-differ-from-pseudo-random-assignment">#</a>
</h3>
<p>Proper randomization provides true independence and unpredictability, while pseudo-random assignments might seem random, but they can introduce subtle biases and patterns that could compromise the validity of the test</p>
<h2 class="heading" id="metrics--measurement">
  Metrics &amp; Measurement
  <a class="anchor" href="#metrics--measurement">#</a>
</h2>
<h3 class="heading" id="what-is-a-north-star-metric-and-why-is-it-important">
  What is a North Star metric and why is it important?
  <a class="anchor" href="#what-is-a-north-star-metric-and-why-is-it-important">#</a>
</h3>
<ul>
<li>A north star or a primary metric is a key measure that best describes the success of the business. Its important because it can be single metric around which different teams can align, and guides the decision making in experiments.</li>
<li>It can indicate the fundamental business objective and, thus indicate overall business health.</li>
</ul>
<h3 class="heading" id="distinguish-between-primary-and-granular-metrics-with-examples">
  Distinguish between primary and granular metrics with examples.
  <a class="anchor" href="#distinguish-between-primary-and-granular-metrics-with-examples">#</a>
</h3>
<ul>
<li>Primary metrics are usually higher-level and best describes the overall business objective. While granular metrics explain user specific behaviors, and can be more sensitive and more actionable.</li>
<li>For example, overall conversion rate can be a primary metric, while sign-up rate, click per visit can be granular metrics.</li>
</ul>
<h3 class="heading" id="what-makes-a-metric-non-gameable">
  What makes a metric &ldquo;non-gameable&rdquo;?
  <a class="anchor" href="#what-makes-a-metric-non-gameable">#</a>
</h3>
<ul>
<li>A metric is non-gameable when it directly reflects the business impact or reflects real user value. It shouldn&rsquo;t be something that can be artificially inflated through superficial changes.</li>
<li>For example, a metric like revenue per visit can be non-gameable, while another metric such as time-on-page can be gamified by adding distracting elements.</li>
</ul>
<h3 class="heading" id="how-do-you-evaluate-if-a-metric-is-stable-sensitive-and-measurable">
  How do you evaluate if a metric is stable, sensitive and measurable?
  <a class="anchor" href="#how-do-you-evaluate-if-a-metric-is-stable-sensitive-and-measurable">#</a>
</h3>
<p>A metric can be considered</p>
<ul>
<li>Stable: If its consistent under normal circumstance, shows minimal random fluctuations, and doesn&rsquo;t get overly affected by external factors</li>
<li>Sensitive: If it responds to relevant changes in user behavior, detects meaningful differences between the variants and changes when actual improvement occurs.</li>
<li>Measurable: If it can be accurately calculated, tracked and logged, and has a clear definition and a calculation method.</li>
</ul>
<h3 class="heading" id="what-are-the-different-types-of-quantitative-metrics-with-examples-meanspercentiles-proportions-ratios">
  What are the different types of quantitative metrics (with examples): Means/percentiles, proportions, ratios?
  <a class="anchor" href="#what-are-the-different-types-of-quantitative-metrics-with-examples-meanspercentiles-proportions-ratios">#</a>
</h3>
<ul>
<li>The means or percentile metrics are measures of central tendency or distribution points. They represent an average or typical value in the original unit of measurement.  For example, a metric like average sales can be obtained by dividing total sales by the number of sales. It returns the value back in the sale amount in dollars.  They preserve the original unit here(dollars), while other metrics (ratios and proportions behave different). Other examples of means metrics include median time on page, or average revenue per user or percentile load times.</li>
<li>Proportions are part of a whole; they must be same unit/type in numerator and denominator. While ratios are basically the comparison between different measures(can be different units/types).</li>
<li>For example, active users/total users(as active users is a subset of total users) is a proportion metric. 500/1000 or 0.5. The value lies between 0-1. Contrast that with a ratio, such as messages per user. For example, 1500 messages/500 users or 3 messages per user. Her we are comparing different measures.</li>
<li>Other examples of proportion measures include signup rate(signups/visits), page abandonment rate(page abandons/total visitors), or conversion rate(purchases/vistors)</li>
<li>Examples or ratio are click through rate(clicks/visits), clicks per ad impression, or pages per visit.</li>
</ul>
<h2 class="heading" id="correlation--statistical-concepts">
  Correlation &amp; Statistical Concepts
  <a class="anchor" href="#correlation--statistical-concepts">#</a>
</h2>
<h3 class="heading" id="what-is-pearsons-correlation-coefficient">
  What is Pearson&rsquo;s correlation coefficient?
  <a class="anchor" href="#what-is-pearsons-correlation-coefficient">#</a>
</h3>
<ul>
<li>Pearson&rsquo;s correlation coefficient is a statistical measure that quantifies the strength of linear relationship between two variables.
It&rsquo;s usually denoted by R. The interpretation of the values:
<ul>
<li>R&gt;0 means that there is a positive correlation</li>
<li>R=0 mesns no correlation</li>
<li>R&lt;0 means negative correlation</li>
</ul>
</li>
<li>Since Pearson&rsquo;s correlation coefficient is a parametric test, it relies on the assumption that the data should be normally distributed</li>
</ul>
<h3 class="heading" id="what-are-the-assumptions-behind-correlation-analysis">
  What are the assumptions behind correlation analysis?
  <a class="anchor" href="#what-are-the-assumptions-behind-correlation-analysis">#</a>
</h3>
<ul>
<li>The data should follow a normal distribution. When the data is non-normal, transformations can be applied to make it normal. Or, there are other tests like Spearman&rsquo;s rank coefficient or Kendall&rsquo;s test.</li>
<li>The data should satisfy the assumption of linearity</li>
</ul>
<h3 class="heading" id="why-doesnt-correlation-always-imply-causation">
  Why doesn&rsquo;t correlation always imply causation?
  <a class="anchor" href="#why-doesnt-correlation-always-imply-causation">#</a>
</h3>
<p>Correlation sits at the bottom of the evidence pyramid, and is grouped with opinions and gut feelings. Observational studies or natural experiments provide stronger evidence, which require some modeling efforts and randomization.
There are often situations where we see a strong correlation, such as page load times and conversion rates, but it doesn&rsquo;t tell us that slow load times is equal to lower conversions, or both are affected by another factor, which is the poor internet connection.</p>
<h2 class="heading" id="evidence-hierarchy">
  Evidence Hierarchy
  <a class="anchor" href="#evidence-hierarchy">#</a>
</h2>
<h3 class="heading" id="describe-the-hierarchy-of-evidence-in-experimental-design">
  Describe the hierarchy of evidence in experimental design.
  <a class="anchor" href="#describe-the-hierarchy-of-evidence-in-experimental-design">#</a>
</h3>
<p>The hierarchy of evidence can be thought of as a pyramid structure of establishing causation.</p>
<ul>
<li>The bottom layer consists of correlations, opinions and gut feelings</li>
<li>The middle layer consists of observational studies/natural experiments, quasi experiments(which require high modeling efforts)</li>
<li>The top layer consists of RCT or Randomly controlled trials, which are like the gold standard in establishing causation.</li>
</ul>
<h3 class="heading" id="what-distinguishes-randomized-controlled-experiments-from-observational-studies">
  What distinguishes randomized controlled experiments from observational studies?
  <a class="anchor" href="#what-distinguishes-randomized-controlled-experiments-from-observational-studies">#</a>
</h3>
<p>RCT&rsquo;s are considered the gold standard because they:</p>
<ul>
<li>create generalizability and representativeness</li>
<li>minimize bias between different groups through randomization</li>
<li>can establish causality by isolating treatment effects</li>
<li>are more trustworthy unlike other methods where randomization and control groups are absent</li>
<li>have greater control unlike observational studies, where certain variables cannot be controlled.</li>
</ul>
<h3 class="heading" id="how-do-quasi-experiments-differ-from-true-experiments">
  How do quasi-experiments differ from true experiments?
  <a class="anchor" href="#how-do-quasi-experiments-differ-from-true-experiments">#</a>
</h3>
<p>True experiments differ from the quasi-experiments in several ways</p>
<ul>
<li>Random assignment: In case of true experiments, there is a complete random assignment to treatment conditions, compared to a quasi-experiment, where groups are formed by natural circumstances or pre-existing conditions</li>
<li>Control over Variables: There is a greater control over the independent variables and the treatment conditions in a true experiment setting, unlike quasi-experiments where researchers have to work with naturally occurring conditions.</li>
<li>Research settings and applicability: True experiments are conducted in controlled environments such as labs or online platforms. While, quasi experiments are carried out in natural settings where full control isn&rsquo;t possible. On the other hand, true experiments are often more feasible for real-world situations where randomizations isn&rsquo;t possible or ethical.</li>
</ul>
<h2 class="heading" id="implementation--practical-considerations">
  Implementation &amp; Practical Considerations
  <a class="anchor" href="#implementation--practical-considerations">#</a>
</h2>
<h3 class="heading" id="what-infrastructure-is-needed-to-implement-ab-testing">
  What infrastructure is needed to implement A/B testing?
  <a class="anchor" href="#what-infrastructure-is-needed-to-implement-ab-testing">#</a>
</h3>
<p>Essential infrastructure needed for A/B testing</p>
<ol>
<li>User assignment system:
<ul>
<li>A robust randomization algorithm and tool to effectively randomize traffic and ensure consistent assignment.</li>
<li>User identification and tracking capabilities(eg UserIds, cookies or session tracking)</li>
</ul>
</li>
<li>Data logging and storage
<ul>
<li>Infrastructure to log hits and clickstream data about user behavior and activities</li>
<li>Data storage solutions for collecting test metrics</li>
</ul>
</li>
<li>Experimentation platform
<ul>
<li>Feature flagging system to control variant deployment</li>
<li>Traffic allocation mechanisms and audience sizing or filtering</li>
<li>Treatment assignment mechanisms to ensure consistent user experience</li>
</ul>
</li>
<li>Analytics infrastucture
<ul>
<li>Statistical analytical tools for hypothesis testing</li>
<li>Real time data processing tools to process metrics and analyze results</li>
<li>Monitoring and visualization tools to real-time experiment tracking and interpreting results</li>
</ul>
</li>
<li>Documentation and Experiment management tools
<ul>
<li>Test configuration management</li>
<li>Experiment tracking and version control</li>
<li>Results documentation and sharing capabilities</li>
</ul>
</li>
</ol>
<h3 class="heading" id="what-are-common-challenges-in-ab-test-implementation">
  What are common challenges in A/B test implementation?
  <a class="anchor" href="#what-are-common-challenges-in-ab-test-implementation">#</a>
</h3>
<p>Common challenges in A/B testing implementation</p>
<ul>
<li>Technical complexity: Ensuring consistent user assignment across sessions; maintaining proper tracking  instrumentations; handling edge cases such as users moving between different platforms; preventing contamination between variants</li>
<li>Sample size and duration trade offs: Balancing the need for sufficient statistical power with business pressures for quick results; especially dealing with limited traffic or trying to detect small changes in metrics which often require large sample sizes.</li>
<li>Data Quality issues: Managing inconsistent tracking; missing data points, or incorrect event logging, while ensuring proper metric computation and avoiding bias from technical issues that might affect one variant differently than other.</li>
</ul>
<h3 class="heading" id="how-do-you-ensure-data-quality-in-ab-testing">
  How do you ensure data quality in A/B testing?
  <a class="anchor" href="#how-do-you-ensure-data-quality-in-ab-testing">#</a>
</h3>
<p>Here are some of the ways to ensure data quality in A/B testing</p>
<ul>
<li>Technical validation: Implement automated checks for data integrity(proper event logging, consistent event tracking, accurate metric computation), using AA testing to verify the experiment setup; and maintaining consistent instrumentation across variants to ensure there&rsquo;s no systematic bias.</li>
<li>Statistical monitoring: Tracking key metrics stability over time; implementing and tracking guardrail metrics to detect anomalies and unexpected changes; and checks for unusual patterns in user behavior or metric distributions that might indicate data quality issues.</li>
<li>Process controls: Document clear metric definitions and logging requirements, and implementing clear procedures for handling data discrepancies or tracking issues when they arise.</li>
</ul>
<h2 class="heading" id="advanced-concepts">
  Advanced Concepts
  <a class="anchor" href="#advanced-concepts">#</a>
</h2>
<h3 class="heading" id="how-do-you-handle-multiple-metrics-in-ab-testing">
  How do you handle multiple metrics in A/B testing?
  <a class="anchor" href="#how-do-you-handle-multiple-metrics-in-ab-testing">#</a>
</h3>
<p>Here are some ways to handle multiple metrics in A/B testing</p>
<ul>
<li>Create a metric hierarchy: Establish a clear structure or hierarchy with the Primary or north start metrics, the secondary or the granular metrics and the guardrail metrics. This helps prioritize decision-making when metrics show conflicting results.</li>
<li>Apply Statistical Rigor: Account for multiple comparison problems when testing several metrics simultaneously, ensure each metric meets requirements of being stable, sensitive, measurable, and non-gameable, and consider the correlation between different metrics when analyzing results</li>
<li>Use Comprehensive Success Criteria: Combine multiple metrics to form more complete success/failure criteria, balance the trade-offs between different metrics (like speed vs accuracy), and ensure metrics together provide a complete picture of the experiment&rsquo;s impact without redundancy</li>
</ul>
<h3 class="heading" id="what-are-guardrail-metrics-and-why-are-they-important">
  What are guardrail metrics and why are they important?
  <a class="anchor" href="#what-are-guardrail-metrics-and-why-are-they-important">#</a>
</h3>
<ul>
<li>Guardrail metrics are safety metrics that ensure core business metrics or user experience aren&rsquo;t negatively impacted by the test variant. While they&rsquo;re not the primary success metrics, they help prevent unintended negative consequences of an experiment.</li>
<li>Help catch potential issues early before they impact the business significantly</li>
<li>Even if your primary test metric improves, a significant drop in guardrail metrics could indicate hidden problems</li>
</ul>
<h3 class="heading" id="how-do-you-balance-speed-of-testing-with-reliability">
  How do you balance speed of testing with reliability?
  <a class="anchor" href="#how-do-you-balance-speed-of-testing-with-reliability">#</a>
</h3>
<p>Here are some ways to balance testing speed with reliablity</p>
<ul>
<li>Statistical rigor vs Time trade-off: Set minimum required sample size based on desired effect size and confidence levels</li>
<li>Efficient test designs: Run tests in parallel when the metrics don&rsquo;t interfere; maintain clear, non-overlapping user groups to avoid interaction effects</li>
<li>Risk-based approach: For low risk changes(UI and copy tweaks) accept lower confidence intervals. For high risk changes, maintain strict statistical requirements.</li>
<li>Use guardrail metrics with strict stopping rules for critical business metrics</li>
<li>Consider the cost of potential errors vs the cost of delayed decisions</li>
</ul>
<h2 class="heading" id="hypothesis-formation">
  Hypothesis Formation
  <a class="anchor" href="#hypothesis-formation">#</a>
</h2>
<h3 class="heading" id="what-is-a-hypothesis-in-the-context-of-ab-testing">
  What is a hypothesis in the context of A/B testing?
  <a class="anchor" href="#what-is-a-hypothesis-in-the-context-of-ab-testing">#</a>
</h3>
<ul>
<li>A hypothesis is a statement that serves as a starting point for further investigation</li>
<li>A strong hypothesis is the one which is
<ul>
<li>testable or measurable</li>
<li>declarative and concise</li>
<li>logical</li>
<li>easy to generalize</li>
<li>results in actionable iterations</li>
</ul>
</li>
<li>Follows a specific format: &ldquo;Based on X, we believe that if we do Y, then Z will happen, as measured by metric M&rdquo;</li>
<li>An example of a hypothesis can be, based on user research, we believe that if we update the recommendation algorithm, then an increase in conversion will happen, as measured by CVR</li>
</ul>
<h3 class="heading" id="how-does-a-null-hypothesis-differ-from-an-alternative-hypothesis">
  How does a null hypothesis differ from an alternative hypothesis?
  <a class="anchor" href="#how-does-a-null-hypothesis-differ-from-an-alternative-hypothesis">#</a>
</h3>
<ul>
<li>Null hypothesis usually states that no changes will occur. For example, the conversion rate will not increase.</li>
<li>However, the alternative hypothesis in this case would be the opposite of null hypothesis, for example, the change would increase the conversion rate.</li>
</ul>
<h2 class="heading" id="statistical-foundations">
  Statistical Foundations
  <a class="anchor" href="#statistical-foundations">#</a>
</h2>
<h3 class="heading" id="what-is-the-central-limit-theorem-and-why-is-it-important-in-ab-testing">
  What is the Central Limit Theorem and why is it important in A/B testing?
  <a class="anchor" href="#what-is-the-central-limit-theorem-and-why-is-it-important-in-ab-testing">#</a>
</h3>
<ul>
<li>CLT states that for a sufficiently large sample size, the distribution of the sample means would be normally distributed around the true population mean, regardless of the distribution of the underlying data.</li>
<li>The standard deviation of this distribution equals the standard error of the mean.</li>
</ul>
<h3 class="heading" id="how-does-standard-deviation-and-standard-error-matter-in-context-of-ab-testing">
  How does standard deviation and standard error matter in context of AB testing.
  <a class="anchor" href="#how-does-standard-deviation-and-standard-error-matter-in-context-of-ab-testing">#</a>
</h3>
<ul>
<li>Standard deviation is the &lsquo;average distance from the mean in your raw data&rsquo;. It measures the spread in the original data. Formula: <code>σ = √(Σ(x - μ)²/N)</code> Variance is the average of squared difference from the mean.  Standard Deviation = √Variance. Variance is always positive, since it is squared. That&rsquo;s the reason, standard deviation is usually used as it is the same units as the original data.</li>
<li>Standard error is &lsquo;how wobbly your sample average is&rsquo;.  Standard Error = Standard Deviation/√n. Since it is inversely proportional to the sample size, the more the sample size is, the smaller the standard error is, and more confident you are in your sample mean.</li>
<li>In A/B testing context, standard deviation show how much the individual users vary. And, the standard error shows how confident you are in the difference of A and B. The smaller the standard error, the better you can detect the small differences between A and B.</li>
<li>Since variance helps calculate the required sample size, and higher variance means more standard error and thus, we need more sample size to detect the small differences between A and B.</li>
</ul>
<h3 class="heading" id="under-what-conditions-does-the-central-limit-theorem-apply">
  Under what conditions does the Central Limit Theorem apply?
  <a class="anchor" href="#under-what-conditions-does-the-central-limit-theorem-apply">#</a>
</h3>
<p>Some key conditions for the CLT to hold include the following</p>
<ul>
<li>Must have sufficiently large sample size(typically &gt;=30 samples)</li>
<li>Observations must be independent and randomly sampled.</li>
<li>The population must have finite variance and mean.</li>
</ul>
<h3 class="heading" id="how-does-sample-size-affect-the-distribution-of-sample-means">
  How does sample size affect the distribution of sample means?
  <a class="anchor" href="#how-does-sample-size-affect-the-distribution-of-sample-means">#</a>
</h3>
<p>As sample size increases:</p>
<ul>
<li>The distribution of the sample means becomes more tightly centered around the true population mean.</li>
<li>The standard error decreases, since it is inversely proportional to the sample size.</li>
<li>The distribution becomes more reliably normal shaped(due to CLT)</li>
</ul>
<h3 class="heading" id="why-is-normal-distribution-important-in-hypothesis-testing">
  Why is normal distribution important in hypothesis testing?
  <a class="anchor" href="#why-is-normal-distribution-important-in-hypothesis-testing">#</a>
</h3>
<ul>
<li>In AB testing, normal distribution is important because it lets us compare the means between the test and the control groups.</li>
<li>The means of two groups, A and B are normally distributed. The difference between normal distributions is also normal. That means the difference between the means(D) also follows a normal distribution.</li>
<li>Under null hypothesis, the distribution(D) centers at zero. (that means no difference between A and B)</li>
<li>The normal distribution of the difference helps us calculate probability of observing differences by chance. It also helps set significance levels and evaluate p-values, and thus make decisions about rejecting or failing to reject the null hypothesis.</li>
</ul>
<h2 class="heading" id="design-parameters-and-error-types">
  Design Parameters and Error Types
  <a class="anchor" href="#design-parameters-and-error-types">#</a>
</h2>
<h3 class="heading" id="what-is-statistical-power-1-β-and-why-is-it-important">
  What is statistical power (1-β) and why is it important?
  <a class="anchor" href="#what-is-statistical-power-1-%ce%b2-and-why-is-it-important">#</a>
</h3>
<ul>
<li>Statistical power(1-β) is defined as the opposite of Type II error or false negative rate. Its commonly set at 80% in AB testing contexts.</li>
<li>Power represents the test&rsquo;s ability to detect true effects when they exist.</li>
<li>For example, if the test has 80% power, it means that if there is a real difference between A and B variants, then you have an 80% chance of detecting it. There&rsquo;s still a 20%(β) chance of missing it Typically 80% is used because this value represents a standard balance between having enough power to detect real effects and keeping  sample size requirements practical.</li>
<li>This is important in the context of AB testing because, it helps determine the sample size and balances practicallity(resources/time) with reliability.</li>
</ul>
<h3 class="heading" id="define-minimum-detectable-effect-mde-and-its-role-in-test-design">
  Define Minimum Detectable Effect (MDE) and its role in test design.
  <a class="anchor" href="#define-minimum-detectable-effect-mde-and-its-role-in-test-design">#</a>
</h3>
<ul>
<li>MDE is defined as the smallest difference that we care to capture. It&rsquo;s one of the key design parameters alongside power and statistical significance. It should be based on historical data from similar tests, from business contexts and requirements, and should also take into account the available resources and traffic.</li>
<li>Higher baseline means smaller relative changes matter, and lower baseline means large relative changes matter.</li>
</ul>
<h3 class="heading" id="what-is-significance-level-α-and-how-is-it-typically-set">
  What is significance level (α) and how is it typically set?
  <a class="anchor" href="#what-is-significance-level-%ce%b1-and-how-is-it-typically-set">#</a>
</h3>
<ul>
<li>α is the Type I error rate and is also the false positive rate. It&rsquo;s commonly set at 5% in AB testing.</li>
<li>What it means in practice is that, its the probability of incorrectly rejecting the null hypothesis. In AB testing context, it would be the probability of declaring that a difference exists when there actually isn&rsquo;t one.</li>
<li>The decision rule is that when the p-value is less than α, we reject the null hypothesis, and when the p-value is greater than α, then we fail to reject the null hypothesis.</li>
</ul>
<h3 class="heading" id="how-do-you-balance-between-type-i-and-type-ii-errors">
  How do you balance between Type I and Type II errors?
  <a class="anchor" href="#how-do-you-balance-between-type-i-and-type-ii-errors">#</a>
</h3>
<ul>
<li>Type I (α) - false positive rate: commonly set at 5%</li>
<li>Type II (β) - false negative rate: power = 1-β, commonly set at 80%. This means β is 20%</li>
<li>Decreasing α (reducing false positives) → Increases β (more false negatives)</li>
<li>Since both of them depend on each other, we can&rsquo;t optimize both without increasing the sample size.</li>
</ul>
<h3 class="heading" id="how-do-you-take-a-decision-based-on-the-p-value">
  How do you take a decision based on the p-value
  <a class="anchor" href="#how-do-you-take-a-decision-based-on-the-p-value">#</a>
</h3>
<ul>
<li>If p-value &lt; α (0.05):
→ Reject null hypothesis
→ &ldquo;We found a significant difference&rdquo;
→ Risk of Type I error is 5%</li>
<li>If p-value &gt; α (0.05):
→ Fail to reject null hypothesis
→ &ldquo;No significant difference found&rdquo;
→ Risk of Type II error depends on effect size</li>
</ul>
<h3 class="heading" id="what-are-the-different-cohens-measures">
  What are the different Cohen&rsquo;s measures?
  <a class="anchor" href="#what-are-the-different-cohens-measures">#</a>
</h3>
<ul>
<li>The purpose of the Cohen&rsquo;s measures is to standardize the effect size across various metrics. It enables sample size calculations, and thus provides a framework for comparing different tests.</li>
<li>Now there are two types of Cohen&rsquo;s measures- Cohen&rsquo;s h for proportions, and Cohen&rsquo;s d for means.
<ul>
<li>Cohen&rsquo;s h is used for proportion metrics such as conversion metrics, click-through rates and things of that nature.</li>
<li>Formula: h = 2 * (arcsin(√P₂) - arcsin(√P₁))
<ul>
<li>Where:
<ul>
<li>P₁ is baseline proportion</li>
<li>P₂ is target proportion</li>
</ul>
</li>
</ul>
</li>
<li>Cohen&rsquo;s d is used for means such as revenue based metrics or time based metrics, for example RPV and average checkout time.</li>
<li>Formula: d = (μ₂ - μ₁)/σ
<ul>
<li>Where:
<ul>
<li>μ₁, μ₂ are means</li>
<li>σ is pooled standard deviation</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 class="heading" id="how-do-you-use-the-cohens-measures-to-measure-the-effect-size">
  How do you use the Cohen&rsquo;s measures to measure the effect size?
  <a class="anchor" href="#how-do-you-use-the-cohens-measures-to-measure-the-effect-size">#</a>
</h3>
<ul>
<li>To interpret the effect size, the following numbers can be referred to
<ul>
<li>Small effect: 0.2</li>
<li>Medium effect: 0.5</li>
<li>Large effect: 0.8</li>
</ul>
</li>
<li>Now once the effect size is calculated, the sample size can be calculated using the following formula
<ul>
<li>n = 2(Zα + Zβ)² / effect_size²</li>
<li>Where:
<ul>
<li>n is sample size per variant</li>
<li>Zα is z-score for significance level (1.96 for α = 5%)</li>
<li>Zβ is z-score for power (0.84 for 80% power)</li>
<li>effect_size is Cohen&rsquo;s h or d</li>
</ul>
</li>
</ul>
</li>
<li>Example:
Baseline: 10% conversion
Target: 11% conversion (10% improvement)
Cohen&rsquo;s h ≈ 0.033
Sample size needed ≈ 14,728 per variant
With 5,000 daily visitors = 6 days test duration</li>
</ul>
<h2 class="heading" id="correction-methods">
  Correction Methods
  <a class="anchor" href="#correction-methods">#</a>
</h2>
<h3 class="heading" id="what-is-the-difference-between-single-comparison-and-multiple-comparison-tests">
  What is the difference between single comparison and multiple comparison tests?
  <a class="anchor" href="#what-is-the-difference-between-single-comparison-and-multiple-comparison-tests">#</a>
</h3>
<ul>
<li>Single comparison tests usually involve comparing A versus just one treatment. They might deal with just a single metric and usually doesn&rsquo;t involve subcategories. An example can be testing a new feature against the existing current feature.</li>
<li>Mutliple comparison tests involve multiple categories and granular categories(A/B/n tests). They may include multiple metrics being tested simultaneously, and thus require correction methods like Bonefferoni methods for correction.</li>
</ul>
<h3 class="heading" id="what-is-family-wise-error-rate-fwer-why-is-fwer-important-in-multiple-comparison-scenarios">
  What is family-wise error rate (FWER)? Why is FWER important in multiple comparison scenarios?
  <a class="anchor" href="#what-is-family-wise-error-rate-fwer-why-is-fwer-important-in-multiple-comparison-scenarios">#</a>
</h3>
<ul>
<li>FWER is defined as the probability of making at least one Type I errors while performing multiple hypothesis tests</li>
<li>Imagine you&rsquo;re running tests on a recommendation module with 4 metrics. If you use the standard significance level of α = 0.05 for each test:
<ul>
<li>For each individual test, you have a 5% chance of a false positive</li>
<li>However, across all tests, the probability of at least one false positive is much higher</li>
<li>The formula for FWER is: FWER = 1 - (1 - α)^m, where m is the number of tests</li>
<li>With 4 tests: FWER = 1 - (1 - 0.05)^4 = 0.19 or 19%</li>
<li>This 19% FWER means:
<ul>
<li>You have a 19% chance of getting at least one false positive across all your tests</li>
<li>This is nearly 4 times higher than your intended error rate of 5%</li>
<li>This inflated error rate could lead to incorrect business decisions</li>
</ul>
</li>
</ul>
</li>
<li>To control FWER, you can use correction methods like:
<ul>
<li>Bonferroni correction: Adjust α to α/m (where m is number of tests)</li>
<li>Šidák correction: A less stringent alternative to Bonferroni</li>
</ul>
</li>
</ul>
<h3 class="heading" id="what-is-the-bonferroni-correction-and-how-is-it-calculated-when-is-this-most-appropriate">
  What is the Bonferroni correction and how is it calculated? when is this most appropriate?
  <a class="anchor" href="#what-is-the-bonferroni-correction-and-how-is-it-calculated-when-is-this-most-appropriate">#</a>
</h3>
<ul>
<li>The Bonferroni correction is the simplest and the most popular approach for controlling the family-wise error rate in multiple correction testing</li>
<li>Here&rsquo;s how it&rsquo;s calculated:
<ul>
<li>Adjusted α = Individual test α / M</li>
<li>Where M is the number of tests being performed</li>
<li>For example, if running 5 tests with α = 0.05:
<ul>
<li>Bonferroni adjusted α = 0.05/5 = 0.01</li>
<li>This means each individual test must achieve p &lt; 0.01 to be considered significant</li>
</ul>
</li>
</ul>
</li>
<li>The Bonferroni correction is most appropriate in these scenarios:
<ul>
<li>When you have a relatively small number of comparisons (typically less than 10)</li>
<li>When controlling Type I errors (false positives) is critical to your business decision</li>
<li>When you need a simple, conservative approach that&rsquo;s easy to explain to stakeholders</li>
<li>When the tests are independent of each other</li>
<li>When the cost of a false positive is high (e.g., major product changes, high-stakes decisions)</li>
</ul>
</li>
</ul>
<h3 class="heading" id="what-is-the-šidák-correction">
  What is the Šidák correction?
  <a class="anchor" href="#what-is-the-%c5%a1id%c3%a1k-correction">#</a>
</h3>
<ul>
<li>The Šidák correction is an alternative method for controlling the family-wise error rate in multiple comparisons.</li>
<li>The formula for Šidák correction is:
<ul>
<li>Adjusted α = 1 - (1 - α)^(1/m)</li>
<li>Where:
<ul>
<li>α is the original significance level (typically 0.05)</li>
<li>m is the number of independent tests</li>
</ul>
</li>
</ul>
</li>
<li>Key differences from Bonferroni:
<ol>
<li>It&rsquo;s slightly less conservative than Bonferroni</li>
<li>It provides exactly the desired family-wise error rate when tests are independent</li>
<li>For the same scenario, Šidák will give a slightly larger adjusted α than Bonferroni</li>
</ol>
</li>
<li>For example, if you&rsquo;re running 4 tests with α = 0.05:
<ul>
<li>Bonferroni correction: 0.05/4 = 0.0125</li>
<li>Šidák correction: 1 - (1 - 0.05)^(1/4) ≈ 0.0127</li>
</ul>
</li>
<li>The Šidák correction is most appropriate when:
<ol>
<li>Tests are truly independent</li>
<li>You want slightly more power than Bonferroni</li>
<li>You need a mathematically exact correction for independent tests</li>
</ol>
</li>
</ul>
<h2 class="heading" id="internal-validation">
  Internal Validation
  <a class="anchor" href="#internal-validation">#</a>
</h2>
<h3 class="heading" id="what-is-sample-ratio-mismatch-srm-and-why-is-it-important">
  What is Sample Ratio Mismatch (SRM) and why is it important?
  <a class="anchor" href="#what-is-sample-ratio-mismatch-srm-and-why-is-it-important">#</a>
</h3>
<ul>
<li>SRM occurs when the allocation of the users across the variants deviates from the intended design ratio in an AB test. For example, if the planned ratio was 50/50 split but got 48/52</li>
<li>SRM is important because it can invalidate the test results by
<ul>
<li>Breaking the fundamental assumptions about random assignment</li>
<li>Introducing a selection bias into the experiment.</li>
<li>Leading to incorrect conclusions about the treatment effects.</li>
</ul>
</li>
</ul>
<h3 class="heading" id="how-do-you-perform-a-chi-square-goodness-of-fit-test-for-srm">
  How do you perform a Chi-square goodness of fit test for SRM?
  <a class="anchor" href="#how-do-you-perform-a-chi-square-goodness-of-fit-test-for-srm">#</a>
</h3>
<ul>
<li>A chi-square goodness of fit test compares the observed user allocated ratios against the expected ration.</li>
<li>This is typically done using the formula
<ul>
<li>Calculate χ² = Σ((Observed - Expected)²/Expected) for each variant</li>
<li>Compare the calculated χ² against critical values.</li>
<li>If p-value &lt; α (typically 0.05), there is evidence of SRM</li>
</ul>
</li>
</ul>
<h3 class="heading" id="what-are-the-common-causes-of-srm">
  What are the common causes of SRM?
  <a class="anchor" href="#what-are-the-common-causes-of-srm">#</a>
</h3>
<p>Some of the common causes of SRM include</p>
<ul>
<li>Assignment problems: incorrect bucketing or randomization algorithm issues</li>
<li>Execution issues: Delayed variant start times or latency in code execution in either control or variant</li>
<li>Data logging issues: Delays in logging data or bot filtering in the traffic</li>
<li>Interference: Experimenter pausing variants during the test duration</li>
</ul>
<h3 class="heading" id="what-is-an-aa-test-and-what-purpose-does-it-serve-and-how-do-you-interpret-its-results">
  What is an A/A test and what purpose does it serve? And, how do you interpret its results.
  <a class="anchor" href="#what-is-an-aa-test-and-what-purpose-does-it-serve-and-how-do-you-interpret-its-results">#</a>
</h3>
<ul>
<li>An A/A test shows identical experiences to two user groups to validate the experiment setup.
<ul>
<li>It serves as a control experiment to detect system bugs</li>
<li>Helps identify imbalances in distribution(browser, devices ets)</li>
<li>Validates randomization and data collection processes</li>
</ul>
</li>
<li>Interpretation guidelines</li>
<li>Significant differences suggest problems with the test setup</li>
<li>The observed difference between the groups represent the natural variation in metrics, and that information can be used to establish baseline variance/noise levels for different metrics.</li>
</ul>
<h2 class="heading" id="external-validation">
  External Validation
  <a class="anchor" href="#external-validation">#</a>
</h2>
<h3 class="heading" id="what-is-simpsons-paradox-and-how-does-it-affect-ab-test-interpretation">
  What is Simpson&rsquo;s Paradox and how does it affect A/B test interpretation?
  <a class="anchor" href="#what-is-simpsons-paradox-and-how-does-it-affect-ab-test-interpretation">#</a>
</h3>
<ul>
<li>It&rsquo;a a statistical phenomenom where trends between variables can emerge, disappear, or even reverse when the population is divided into segments. For example, a feature may show positive results overall, but when it is divided into different user segments, the results can be negative.</li>
<li>If can affect the test interpretation by masking true treatment effects within specific segments.</li>
<li>To mitigate these issues, we need to analyze results both at aggregate and segment levels. Also, we need to understand key user segments before running tests, and investigate unexpected discrepancies between overall and segmented results.</li>
</ul>
<h3 class="heading" id="define-novelty-effect-and-its-impact-on-test-results">
  Define novelty effect and its impact on test results
  <a class="anchor" href="#define-novelty-effect-and-its-impact-on-test-results">#</a>
</h3>
<ul>
<li>Novelty effect is a short-lived improvement in metrics caused by users&rsquo; curiosity about a new feature. This can cause the results to temporarily show an upward trend, when in reality, that might not be the long-term behavior</li>
<li>It can impact the test results by leading to falsely positive initial results that decay over time.</li>
<li>Some ways to address novelty effect can be increasing the test duration so that the effects stabilize over time, or even including the data for analyses after stabilization. Examining new and return user behavior separately can also be another way to mitigate this issue.</li>
</ul>
<h3 class="heading" id="what-is-change-aversion-and-how-does-it-differ-from-novelty-effect">
  What is change aversion and how does it differ from novelty effect?
  <a class="anchor" href="#what-is-change-aversion-and-how-does-it-differ-from-novelty-effect">#</a>
</h3>
<ul>
<li>Change aversion is the opposite of novelty effect- when users avoid trying out a new feature due to familiarity with the old feature, even if the new feature might be better for them.</li>
<li>This can also impact the test results by leading to falsely negative initial results, which may improve over time.</li>
</ul>
<h2 class="heading" id="statistical-analysis---proportions">
  Statistical Analysis - Proportions
  <a class="anchor" href="#statistical-analysis---proportions">#</a>
</h2>
<p>Explain the framework for analyzing differences in proportions?</p>
<ul>
<li>For sufficiently large samples, the sampling distribution of differences in proportions follows a normal distribution. This allows us to use a z test to compare proportions between groups.</li>
<li>The statistical test framework is set up in the following way
<ul>
<li>Null hypothesis(H₀): No difference between proportions(p₁ - p₂ = 0)</li>
<li>Alternate hypothesis(H₁): There is a difference(p₁ - p₂ ≠ 0)</li>
<li>Test Statistic: Calculate the z score using the difference in proportions and the standard error</li>
<li>Compare the p-values against the significance level to make decision.</li>
</ul>
</li>
</ul>
<h3 class="heading" id="what-are-confidence-intervals-and-why-are-they-important">
  What are confidence intervals and why are they important?
  <a class="anchor" href="#what-are-confidence-intervals-and-why-are-they-important">#</a>
</h3>
<ul>
<li>The confidence intervals provide a range of feasible values rather than a single point estimate. The 95% CI, for example is a range that captures the true difference between the variants 95% of the time.</li>
<li>Its centered around the observed difference(what we actually measure in our A/B test), and aims to capture the true difference.
CI = Observed Difference ± (Critical Value × Standard Error)</li>
<li>As the sample size increases, the standard error decreases, and that causes the CI to shrink.</li>
</ul>
<h3 class="heading" id="how-do-you-perform-a-two-sample-proportions-z-test">
  How do you perform a two-sample proportions z-test?
  <a class="anchor" href="#how-do-you-perform-a-two-sample-proportions-z-test">#</a>
</h3>
<p>Let&rsquo;s try to illustrate the test using an example</p>
<ul>
<li>Observed Differences:
<ul>
<li>Control CVR: 4.3295%</li>
<li>Variant CVR: 4.3398%</li>
<li>Observed Absolute Difference: 0.0103% (this is our point estimate)</li>
</ul>
</li>
<li>Confidence Interval Analysis:
<ul>
<li>95% CI: [-0.0181%, 0.0388%]</li>
<li>The CI is centered around our observed difference (0.0103%)</li>
<li>The width of the interval (about 0.057 percentage points) indicates our precision</li>
<li>Since the CI contains 0, we cannot be confident that there&rsquo;s a true difference between variants</li>
</ul>
</li>
<li>Statistical Significance:
<ul>
<li>P-value: 0.4761 &gt; 0.05</li>
<li>Not statistically significant</li>
<li>This aligns with our CI containing 0</li>
</ul>
</li>
</ul>
<h3 class="heading" id="what-does-it-mean-when-the-ci-contains-a-0">
  What does it mean when the CI contains a 0?
  <a class="anchor" href="#what-does-it-mean-when-the-ci-contains-a-0">#</a>
</h3>
<ul>
<li>In an AB test, 0 represents &ldquo;no difference&rdquo; between variants</li>
<li>If 0 falls within our CI, it means that &rsquo;no difference&rsquo; is a plausible scenario.</li>
</ul>
<h3 class="heading" id="what-factors-affect-the-width-of-confidence-intervals">
  What factors affect the width of confidence intervals?
  <a class="anchor" href="#what-factors-affect-the-width-of-confidence-intervals">#</a>
</h3>
<p>Here are some of the key factors that affect the width of the CI</p>
<ul>
<li>Sample Sizes Larger sample sizes would lead to narrower Confidence intervals. This is because, more data reduces the standard error, and subsequently reduces CI</li>
<li>Variance/Standard Error: Higher variance in data would lead to higher standard error, and thus the CI would increase as a result</li>
<li>Confidence Level: Higher confidence levels such as 95% or 99% lead to wider intervals as the critical value increases, which leads to CI getting wider.</li>
</ul>
<h3 class="heading" id="how-does-the-confidence-level-affect-the-width-of-the-confidence-interval">
  How does the confidence level affect the width of the confidence interval?
  <a class="anchor" href="#how-does-the-confidence-level-affect-the-width-of-the-confidence-interval">#</a>
</h3>
<ul>
<li>We can think of it as a trade off between confidence and precision. 95% CI means that we are 95% confident that the true value falls within our interval. Similarily, 99% confident would mean that we are 99 % confident that the true value falls within our interval.</li>
<li>To be more confident, we would need to cast a wider net. CI can be thought of as fishing with different sized nets. A smaller net(95% CI) might miss some fish but it gives a more precise location(of the true difference). Similarily, a larger net(99% CI) catches more fish, but tell us less precisely where they are.</li>
</ul>
<h2 class="heading" id="statistical-analysis---means">
  Statistical Analysis - Means
  <a class="anchor" href="#statistical-analysis---means">#</a>
</h2>
<h3 class="heading" id="what-is-the-framework-for-analyzing-differences-in-means">
  What is the framework for analyzing differences in means?
  <a class="anchor" href="#what-is-the-framework-for-analyzing-differences-in-means">#</a>
</h3>
<ul>
<li>According to CLT, for sufficiently large samples, the difference in means follows a normal distribution. This allows us to use t-tests to compare the means, and calculate CI intervals, and then determine if the observed difference is significant or not.</li>
<li>For this, we assume that the Null Hypothesis() is true(no difference exists). Then, we calculate how likely it would it be to observe our sample difference if H0 were true. If this probability is very small, we reject the H0, suggesting that the difference is real.</li>
</ul>
<h3 class="heading" id="when-should-you-use-a-t-test-versus-other-statistical-tests">
  When should you use a t-test versus other statistical tests?
  <a class="anchor" href="#when-should-you-use-a-t-test-versus-other-statistical-tests">#</a>
</h3>
<ul>
<li>We should be using the t-test when we are comparing the means between the groups(not proportions or ranks)</li>
</ul>
<h2 class="heading" id="parametric-tests">
  Parametric Tests
  <a class="anchor" href="#parametric-tests">#</a>
</h2>
<h3 class="heading" id="what-are-parametric-and-non-parametric-tests">
  What are parametric and non-parametric tests
  <a class="anchor" href="#what-are-parametric-and-non-parametric-tests">#</a>
</h3>
<ul>
<li>The term parameter comes from the parameter estimation of population distributions. They are called &ldquo;parametric&rdquo; because they assume that the data follows a specific probability distribution(usually normal) and that can be described by a fixed set of parameters. For example, a normal distribution can be explained using two parameters which are μ (population mean) and σ (population standard deviation). For these, distributions, there are parametric tests such as t-tests, z-tests, ANOVA. We estimate these parameters from the sample data and then use them in our statistical inference.</li>
<li>On the other hand, the non-parametric tests don&rsquo;t assume that the data follows any specific type of distribution shape. Instead of using parameters of a distribution, these tests rely on ranks, frequencies, and other order statistics. The non parametric tests are more flexible, but are less powerful than the parametric tests. They are particularly useful for skewed data, ordinal data, or cases when the parametric assumptions are violated. Examples include, Mann Whitney, Chi-square tests.</li>
</ul>
<h3 class="heading" id="what-are-the-key-assumptions-of-parametric-tests">
  What are the key assumptions of parametric tests?
  <a class="anchor" href="#what-are-the-key-assumptions-of-parametric-tests">#</a>
</h3>
<p>The key assumptions of parametric tests are as follows</p>
<ul>
<li>Random Sampling: Data must be randomly sampled from the population, which ensures representativenss and generalizability.</li>
<li>Independence: Each observation or data point must be independent of others. This is critical because not accounting for dependencies inflates error rates. For example, if the same user appears multiple times in the test, observations aren&rsquo;t independent.</li>
<li>Normality: Data should follow a normal distribution. However, this can be relaxed for large enough sample sizes due to Central Limit Theorem.</li>
</ul>
<h3 class="heading" id="how-do-you-verify-independence-in-observations">
  How do you verify independence in observations?
  <a class="anchor" href="#how-do-you-verify-independence-in-observations">#</a>
</h3>
<p>There are ways to verify independence in observations for AB tests</p>
<ul>
<li>Through experimental design review:
<ul>
<li>Checking the randomization unit(how users are assigned to a variant)</li>
<li>Ensuring that each user is only counted once in an experiment</li>
<li>Looking for proper user identification and tracking mechanisms</li>
</ul>
</li>
<li>Through data analysis
<ul>
<li>Checking for duplicate user IDs or session IDs</li>
<li>Looking for temporal dependencies(autocorrelation) in the metrics</li>
<li>Verifying that the user behaviour in one group is not affecting the user behaviour in the other group</li>
</ul>
</li>
<li>Other cases to watch out for
<ul>
<li>Network effects</li>
<li>Same user being counted multiple times</li>
<li>Session level metrics being treated as independent when they are coming from the same user</li>
<li>Cross device usage where the same user appears as different users.</li>
<li>Seasonal or time based dependencies in the data.</li>
</ul>
</li>
</ul>
<h3 class="heading" id="how-do-violations-of-these-assumptions-affect-test-results">
  How do violations of these assumptions affect test results?
  <a class="anchor" href="#how-do-violations-of-these-assumptions-affect-test-results">#</a>
</h3>
<ul>
<li>Violation of Random sampling: They can lead to selection bias. The results also would fail to be generalizable to the whole population.</li>
<li>Violation of Independence: This might inflate the error rates(both Type I and Type II). This underestimates the standard error, making the appear more significant than they really are. For example, network effects might make the treatments appear more effective. Mutliple counts of the same user can also inflate sample size, which can make the standard error artificially smaller. This leads to a smaller p-value than we should have, and might show statistical significance where there isn&rsquo;t any.</li>
<li>Violation of Normality: Less severe for large samples(n&gt;=30) due to CLT. This can be more severe if the data is more skewed or has extreme outliers.</li>
</ul>
<h2 class="heading" id="ratio-metrics">
  Ratio Metrics
  <a class="anchor" href="#ratio-metrics">#</a>
</h2>
<h3 class="heading" id="what-is-the-delta-method-and-why-is-it-important">
  What is the delta method and why is it important?
  <a class="anchor" href="#what-is-the-delta-method-and-why-is-it-important">#</a>
</h3>
<ul>
<li>The delta method is a statistical technique used specifically for analyzing ratio metrics in AB testing- when you have a numerator and a denominator(For example, revenue per user, or clicks per session)</li>
<li>The delta methods helps handle the additional complexity, that arises because the numerator and the denominator can vary independently. Standard tests like the t-tests don&rsquo;t account for the correlation between numerator and the denominator, and can&rsquo;t handle the variance structure of the ratios.</li>
<li>The delta method solves this issue by providing a way to approximate the variance of a function(in this case, a ratio) of random variables, and taking into account the covariance between the numerator and the denominator.</li>
<li>For example, two scenarios with $10 RPU can be statistically different due to their underlying variability: $1000/100 users might have a confidence interval of $10 ± $1, while $900/90 users might show $10 ± $3 due to different variances in revenue and user counts.</li>
</ul>
<h3 class="heading" id="what-is-the-unit-of-analysis-and-why-does-it-matter">
  What is the unit of analysis and why does it matter?
  <a class="anchor" href="#what-is-the-unit-of-analysis-and-why-does-it-matter">#</a>
</h3>
<ul>
<li>Unit of analysis is the entity that is being analyzed in an AB test(like users, sessions, or pageviews)- essentially what you are measuring in your metrics.</li>
<li>This matters because, it determines how you calculate your metrics and their variance. This must be carefully considered when dealing with ratio metrics, where the numerator and the denominator might have different units.</li>
</ul>
<h3 class="heading" id="how-do-you-estimate-variance-for-ratio-metrics">
  How do you estimate variance for ratio metrics?
  <a class="anchor" href="#how-do-you-estimate-variance-for-ratio-metrics">#</a>
</h3>
<ul>
<li>Using delta method, we can properly account for the variance in the numerator and the denominator.</li>
<li>For a ratio R = X/Y, the variance of R is approximately: Var(R) ≈ (1/μy²)[σx² + R²σy² - 2Rσxy]</li>
<li>This can be then used in z-tests. z = (R₁ - R₂)/√(Var(R₁) + Var(R₂))</li>
</ul>
<h2 class="heading" id="best-practices">
  Best Practices
  <a class="anchor" href="#best-practices">#</a>
</h2>
<h3 class="heading" id="why-should-you-avoid-peeking-at-test-results-early">
  Why should you avoid peeking at test results early?
  <a class="anchor" href="#why-should-you-avoid-peeking-at-test-results-early">#</a>
</h3>
<ul>
<li>Peeking at results before reaching the designed sample size inflates the error rates, because when you repeatedly check results, you are effectively doing multiple hypothesis tests, which increases the chances of false positives.</li>
<li>With each peek, you increase the probability of seeing a &ldquo;significant&rdquo; result by chance, even if there&rsquo;s no real difference.</li>
<li>For example if you peek 5 times during the test, your actual false positive rate jumps to around 14%. This means you&rsquo;re almost 3 times more likely to conclude there&rsquo;s an improvement when there isn&rsquo;t one.</li>
</ul>
<h3 class="heading" id="how-do-you-account-for-day-of-week-effects">
  How do you account for day-of-week effects?
  <a class="anchor" href="#how-do-you-account-for-day-of-week-effects">#</a>
</h3>
<ul>
<li>As the users behave differently on weekends vs the weekdays, the test duration should include complete weeks of data to capture the overall behaviour patterns.</li>
<li>Some practices around this can be to run the tests for complete weeks(2 weeks instead of 10 days).</li>
<li>This is more pronounced in B2B sites, ecommerce websites</li>
</ul>
<h3 class="heading" id="what-are-painted-door-tests-and-when-should-they-be-used">
  What are painted door tests and when should they be used?
  <a class="anchor" href="#what-are-painted-door-tests-and-when-should-they-be-used">#</a>
</h3>
<ul>
<li>Painted door tests are experiments where you test the user intent in a feature before building it. For example, adding a button for a new feature, but it&rsquo;s not functional yet.</li>
<li>Since this measures intent/interest rather than actual usage, it can help validate the demand before putting efforts to actually build it.</li>
</ul>
<h3 class="heading" id="why-is-isolation-important-in-ab-testing">
  Why is isolation important in A/B testing?
  <a class="anchor" href="#why-is-isolation-important-in-ab-testing">#</a>
</h3>
<p>Isolation in AB testing is essentially testing one feature at a time. This is crucial because:</p>
<ul>
<li>Attribution Clarity: When testing multiple changes, its hard to determine which change caused the desired effect</li>
<li>Clean analysis: Isolated changes provide clear cause and effect relationships for the metrics.</li>
<li>Debugging: If something goes wrong, its easier to identify the problematic change when the variables are isolated.</li>
</ul>

    </div>
  </article>

  

  

  
  

  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


    </main>
  </div>

  
  





    




  <footer>
    

    
    





    




    
    <p>Powered by
        <a href="https://gohugo.io/">Hugo</a>
        and
        <a href="https://github.com/tomfran/typo">tomfran/typo</a>
    </p>
    
    
    


  </footer>

  
</body>

<script src="/js/theme-switch.js"></script>
<script defer src="/js/copy-code.js"></script>
</html>
